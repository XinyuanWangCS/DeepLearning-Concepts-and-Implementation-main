{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 卷积神经网络CNN\n",
    "\n",
    "## 4.1 基础知识介绍\n",
    "\n",
    "### 4.1.1 卷积\n",
    "\n",
    "​\t我们知道深度学习是一种表示学习，通过神经网络来提取数据中的特征，卷积神经网络也是如此，而卷积操作就是CNN提取数据体征的手段。\n",
    "\n",
    "####  1. 卷积的定义 \n",
    "\n",
    "​\t卷积在数学中是一种特殊的运算，表示为$s(t)=(x*w)(t)$，x为输入，函数w为核函数，输出被称为特征映射(feature map)，它的离散表示被定义为：![image-20210503175508437](./imgs/图片1.png)\n",
    "\n",
    "其含义是：核函数在原函数上滑动，特征映射输出中的每个值即每次滑动中核函数与原函数对应值相乘之和。注意到卷积公式，我们对核进行了翻转在与输入相乘，但在神经网络中我们省去了翻转，事实上未翻转的运算被称为相关，如下图：\n",
    "\n",
    "<img src=\"./imgs/图片2.png\" alt=\"image-20210708113235088\" style=\"zoom:80%;\" />\t\n",
    "\n",
    "卷积同样可用于二维数据，如图像$I$，与二维的核$K$进行卷积：![image-20210503175516460](./imgs/图片3.png)\n",
    "\n",
    "https://www.zhihu.com/question/22298352\n",
    "\n",
    "![img](./imgs/图片4.gif)\t\n",
    "\n",
    "![image-20210708112856145](./imgs/图片5.png)\n",
    "\n",
    "#### 2. 卷积的含义\n",
    "\n",
    "​\t那么为什么卷积网络中使用卷积可以提取图像信息呢？我们从图像处理角度理解这个问题。我们知道图像就是一个矩阵，当我们使用不同的核（滤波器）进行卷积时，我们可以得到包含不同信息的输出。例如我们有图片：\n",
    "\n",
    "<img src=\"./imgs/图片6.png\" alt=\"image-20210503174906511\" style=\"zoom:80%;\" />\n",
    "\n",
    "​\t以及卷积核： $[[-1,0,1],[-1,0,1],[-1,0,1]]$ ，它的意义是图像左边的像素减去右边的像素，也就是图像水平方向的梯度。当应用此卷积核，我们便得到了图像竖直方向的纹理信息：\n",
    "\n",
    "<img src=\"./imgs/图片7.png\" alt=\"image-20210503175051782\" style=\"zoom:80%;\" />\n",
    "\n",
    "\n",
    "\n",
    "​\t同样，我们使用卷积核 $[[-1,-1,-1],[0,0,0],[1,1,1]]$ ，就可以得到水平纹理信息\n",
    "\n",
    "<img src=\"./imgs/图片8.png\" alt=\"image-20210503175132284\" style=\"zoom:80%;\" />\n",
    "\n",
    "​\t当我们有许多不同的卷积核时，我们就可以从图像中提取出图像隐藏的不同信息，在卷积网络中存每个神经元都是不同的卷积核，提取不同的图像信息，这也是为什么卷积网络可以拟合图像数据，每个卷积核卷积得到的输出叫做**特征图 feature map**。\n",
    "\n",
    "![image-20210708113151961](./imgs/图片9.png)\n",
    "\n",
    "\n",
    "\n",
    "#### 3. 卷积的优点：\n",
    "\n",
    "\t1. 保持了空间结构：卷积的滑动窗口的计算方式可以保留原数据的空间信息，如图片经过二维卷积仍是二维矩阵；\n",
    "\n",
    "![image-20210708174854518](./imgs/图片10.png)\n",
    "\n",
    "\t2. 减少参数量：对于28*28*3大小的图片，如果使用全连接层进行10分类，那么需要的参数量是28*28*3*10，而如果使用10个3*3的卷积核，仅需要10*3*3的参数。\n",
    "\n",
    "<img src=\"./imgs/图片11.png\" alt=\"image-20210708175308866\" style=\"zoom:80%;\" />\n",
    "\n",
    "#### 4. 卷积的特点\n",
    "\n",
    "\t1. 离散连接 Sparse connections：单个卷积的输出仅与特定位置的输入有关，相比全连接网络，每个输出都与全部输入有关；\n",
    "\n",
    "<img src=\"./imgs/图片12.png\" alt=\"image-20210708180007939\" style=\"zoom:80%;\" />\n",
    "\n",
    "\t2. 参数共享 Parameter sharing：单个卷积核被多个输入共享，例如下图，对于加粗权重由于滑动，被所有输入共享，而全连接网络每个输入输出的权重都是一一对应的；\n",
    "\n",
    "<img src=\"./imgs/图片13.png\" alt=\"image-20210708180309478\" style=\"zoom:80%;\" />\n",
    "\n",
    "\t3. 等变表示 Equivariant representations：对输入的空间变换有泛化性。\n",
    "\n",
    "#### 5. 相关概念\n",
    "\n",
    "##### \t5.1 感受野 receptive filed：\n",
    "\n",
    "​\t\t一个卷积核能跨越（看到）的输入的大小，在网络中感受野逐渐增大：\n",
    "\n",
    "<img src=\"./imgs/图片14.png\" alt=\"image-20210708193919678\" style=\"zoom:80%;\" />\n",
    "\n",
    "##### \t5.2 步长 stride\n",
    "\n",
    "​\t\t在卷积的过程中，我们有时会为了减少计算量，改变卷积过程中滑动窗口每次跨越的距离，如图卷积的步长为2：\n",
    "\n",
    "<img src=\"./imgs/图片15.png\" alt=\"image-20210708194101002\" style=\"zoom:80%;\" />\n",
    "\n",
    "##### \t5.3 填充 padding\n",
    "\n",
    "​\t\t如果我们不对卷积的输入做任何操作，那么对于输入大小为6，卷积核大小为3的卷积输出大小为4，通常为了消除这样的输出尺度减小或人为地控制输出的大小，我们会对卷积输入进行填充，最常用的是**0填充(zero padding)**。如下图，使用0对上层输入进行填充，使所有的输出都是相同的大小：\n",
    "\n",
    "<img src=\"./imgs/图片16.png\" alt=\"image-20210708194537926\" style=\"zoom:80%;\" />\n",
    "\n",
    "​\t有三种填充方式：\n",
    "\n",
    "https://blog.csdn.net/suiyueruge1314/article/details/104949254\n",
    "\n",
    "​\t（1） Valid Padding：不做填充，输入m，卷积核k，输出大小为m-k+1;\n",
    "\n",
    "<img src=\"./imgs/图片17.png\" alt=\"image-20210708195039387\" style=\"zoom:30%;\" />\n",
    "\n",
    "​\t（2）Same Padding：进行填充，使输入m，输出大小也为m；\n",
    "\n",
    "<img src=\"./imgs/图片18.png\" alt=\"image-20210708195025479\" style=\"zoom:30%;\" />\n",
    "\n",
    "​\t（3）Full Padding：每个输入都对等量的输出有贡献，输入m，输出m+k-1\n",
    "\n",
    "<img src=\"./imgs/图片19.png\" alt=\"image-20210708195010015\" style=\"zoom:30%;\" />\n",
    "\n",
    "### 4.1.2 池化\n",
    "\n",
    "#### 1. 池化的定义\n",
    "\n",
    "<img src=\"./imgs/图片20.png\" alt=\"image-20210503175628499\" style=\"zoom:80%;\" />\n",
    "\n",
    "在卷积神经网络中，除了卷积层，还有一个不可或缺的部分叫做池化层(pooling layer)，其主要目的是尽量保留原有信息的同时，将数据的尺寸降低。池化是一种运算，它不包含任何参数，池化有许多形式，如平均池化，最大池化等，我们介绍**最大池化**。\n",
    "\n",
    "<img src=\"./imgs/图片21.png\" alt=\"image-20210503180136665\" style=\"zoom:80%;\" />\n",
    "\n",
    "如图所示，在原图像上使用（2,2）的最大池化，即在每个（2,2）范围中选出最大值作为输出，图像的尺寸也被缩小为1/2。\n",
    "\n",
    "#### 2. 池化的意义\n",
    "\n",
    "​\t池化的意义除了可以缩减输出尺寸外，池化还具有局部等变性，因为池化操作只选择局部中的特定值，所以输入的局部变化不会影响池化结果，比如人的眼睛总是在人脸的上方，但有点人眼睛偏高一些，有的人偏低，通过池化我们总是能挑出人脸上方的眼睛，这不会收到其局部位置的差别而改变。\n",
    "\n",
    "1. 不变性：\n",
    "\n",
    "   （1） 使网络对输入的小变换的鲁棒性更高，意思是尽管输入进行了一些改变，网络的输出仍是不变的；\n",
    "\n",
    "   （2） 局部特征具有一定的空间不变性，如上文所述；\n",
    "\n",
    "   （3）池化可以被看做一个无限强的先验，令网络必须对微小变换具有不变性。\n",
    "\n",
    "2. 高效性：池化可以将k个单元压缩为1个单元，减少了网络的计算量和内存要求\n",
    "\n",
    "<img src=\"./imgs/图片22.png\" alt=\"image-20210708191644062\" style=\"zoom:80%;\" />\n",
    "\n",
    "## 4.2 卷积神经网络 CNN\n",
    "\n",
    "### 4.2.1 卷积神经网络概述\n",
    "\n",
    "<img src=\"./imgs/图片23.png\" alt=\"image-20210708192744803\" style=\"zoom:80%;\" />\t\n",
    "\n",
    "​\t卷积神经网络与多层感知机相似，网络由最基本的模块（卷积层、非线性激活层、池化层）重复堆叠而成，在网络的末尾，针对我们的任务的不同，会拼接上不同的输出层，比如如果是分类任务，其输出层就是全连接层+softmax层。\n",
    "\n",
    "<img src=\"./imgs/图片24.png\" alt=\"image-20210708193021240\" style=\"zoom:80%;\" />\n",
    "\n",
    "​\t\n",
    "\n",
    "在网络逐渐深入的过程中，网络从数据中提取的信息逐渐从低层次语义信息，变为高级语义信息：\n",
    "\n",
    "<img src=\"./imgs/图片25.png\" alt=\"image-20210708193159798\" style=\"zoom:80%;\" />\n",
    "\n",
    "### 4.2.2 卷积网络的多层卷积和3D特征映射\n",
    "\n",
    "​\t在卷积网络中，一个卷积层的输入通常是3维的 $(C\\times H\\times W)$ ，例如一张彩色图片包含3个通道，其大小为 $3*32*32$ ；\n",
    "\n",
    "<img src=\"./imgs/图片26.png\" alt=\"image-20210708195907703\" />\n",
    "\n",
    "​\t而一个卷积层包含多个卷积核，每个卷积核对输入进行卷积可以得到一个通道的输出，因此卷积层的输出也是3维的$(C\\times H\\times W)$，其中通道数$C$对应卷积层的卷积核数量：\n",
    "\n",
    "![image-20210708200030708](./imgs/图片27.png)\n",
    "\n",
    "​\t为了令卷积核可以学习到多个通道的信息，通常我们使用3维的卷积核，大小为$(in\\_channels, h, w)$，in_channels对应上层输出的通道数，所以单个卷积层的参数量为卷积核数量*卷积核大小：$(out\\_channels, in\\_channels, h, w)$\n",
    "\n",
    "<img src=\"./imgs/图片28.png\" alt=\"image-20210708200525683\" style=\"zoom:80%;\" />\n",
    "\n",
    "### 4.2.3 经典的卷积神经网络\n",
    "\n",
    "#### 1. AlexNet\n",
    "\n",
    "![image-20210708200701934](./imgs/图片29.png)\n",
    "\n",
    "#### 2. VGGNet\n",
    "\n",
    "<img src=\"./imgs/图片30.png\" alt=\"image-20210708200748137\" style=\"zoom:80%;\" />\n",
    "\n",
    "#### 3. GoogLeNet\n",
    "\n",
    "<img src=\"./imgs/图片31.png\" alt=\"image-20210708200825170\" style=\"zoom:80%;\" />\n",
    "\n",
    "#### 4. ResNet\n",
    "\n",
    "<img src=\"./imgs/图片32.png\" alt=\"image-20210708200858035\" style=\"zoom:80%;\" />\n",
    "\n",
    "<img src=\"./imgs/图片33.png\" alt=\"image-20210708200920154\" style=\"zoom:80%;\" />\n",
    "\n",
    "#### 5. DenseNet\n",
    "\n",
    "<img src=\"./imgs/图片34.png\" alt=\"image-20210708200949735\" style=\"zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 设计一个卷积层  \n",
    "我们自己实现一个卷积函数，然后通过Pytorch的包设计一个卷积层类。\n",
    "![image-20210708112856145](./imgs/图片5.png)\n",
    "卷积函数输入为图像X和卷积核K，我们使用循环遍历X，令卷积核与对应位置的像素相乘求和，从而实现卷积函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "def corr2d(X, K):  #\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造如上图的卷积核和输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = torch.tensor([[0, 1], [2, 3]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们来实现**卷积层**：一个卷积层包括的参数有$w$和偏置$b$，当我们的类继承了nn.Module类后，我们就可以在__init__函数中声明我们的参数，然后在前向传播函数中使用卷积函数corr2d完成卷积层的前向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 用Pytorch实现卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 导入所需包，加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "# 加载数据集\n",
    "training_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 定义CNN模型  \n",
    "    1. 模型包含两部分：特征提取部分，分类器部分；\n",
    "    2. 特征提取部分是一个Sequential类，支持创建序列层叠式的模型，由卷积层、ReLU层、池化层堆叠而成；\n",
    "    3. 卷积层为Conv2d类，参数为输入通道、输出通道、卷积核大小、步长、填充，MNIST图像通道数为1；\n",
    "    4. 池化层为MaxPool2d类，池化窗口大小为2*2；\n",
    "    5. 分类器由全连接层，ReLU层，Dropout2d层组成，最终输出大小为分类数10；\n",
    "    6. 在前向传播forward函数中，输入图像先通过特征提取器，然后扁平化，再通过分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class CNN_Model(nn.Module):\n",
    "    '''Model类包含net, classifier两部分，输出为10类'''\n",
    "    def __init__(self, cls):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=(1,1), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=(1,1), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=(1,1), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=(1,1), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=(1,1), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128*7*7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "            nn.Linear(512, cls),\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  CNN_Model(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=6272, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout2d(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看网络结构\n",
    "model = CNN_Model(10).cuda()\n",
    "print(\"Model structure: \", model, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 定义超参数，损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epoch = 10\n",
    "\n",
    "#定义损失函数和优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练、测试\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch,  (x, y) in enumerate(dataloader):\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        pred = model(x)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*len(x)\n",
    "            print(f'loss:{loss:>7f} [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, accuracy = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            accuracy += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    accuracy /= size\n",
    "    print(f'Accuracy: {(100*accuracy):>0.1f}%, Average Loss: {test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss:2.301050 [    0/60000]\n",
      "loss:0.182495 [ 6400/60000]\n",
      "loss:0.111389 [12800/60000]\n",
      "loss:0.095940 [19200/60000]\n",
      "loss:0.147666 [25600/60000]\n",
      "loss:0.064339 [32000/60000]\n",
      "loss:0.048083 [38400/60000]\n",
      "loss:0.089041 [44800/60000]\n",
      "loss:0.226864 [51200/60000]\n",
      "loss:0.085935 [57600/60000]\n",
      "Accuracy: 97.2%, Average Loss: 0.001369\n",
      "\n",
      "Epoch 1:\n",
      "loss:0.016799 [    0/60000]\n",
      "loss:0.251648 [ 6400/60000]\n",
      "loss:0.029700 [12800/60000]\n",
      "loss:0.055928 [19200/60000]\n",
      "loss:0.029264 [25600/60000]\n",
      "loss:0.012395 [32000/60000]\n",
      "loss:0.037679 [38400/60000]\n",
      "loss:0.035719 [44800/60000]\n",
      "loss:0.116514 [51200/60000]\n",
      "loss:0.047462 [57600/60000]\n",
      "Accuracy: 98.6%, Average Loss: 0.000710\n",
      "\n",
      "Epoch 2:\n",
      "loss:0.021571 [    0/60000]\n",
      "loss:0.038769 [ 6400/60000]\n",
      "loss:0.044400 [12800/60000]\n",
      "loss:0.001043 [19200/60000]\n",
      "loss:0.008311 [25600/60000]\n",
      "loss:0.012514 [32000/60000]\n",
      "loss:0.054659 [38400/60000]\n",
      "loss:0.027045 [44800/60000]\n",
      "loss:0.024772 [51200/60000]\n",
      "loss:0.097816 [57600/60000]\n",
      "Accuracy: 99.0%, Average Loss: 0.000545\n",
      "\n",
      "Epoch 3:\n",
      "loss:0.002162 [    0/60000]\n",
      "loss:0.010881 [ 6400/60000]\n",
      "loss:0.023143 [12800/60000]\n",
      "loss:0.159117 [19200/60000]\n",
      "loss:0.043770 [25600/60000]\n",
      "loss:0.000901 [32000/60000]\n",
      "loss:0.018079 [38400/60000]\n",
      "loss:0.011453 [44800/60000]\n",
      "loss:0.194627 [51200/60000]\n",
      "loss:0.060634 [57600/60000]\n",
      "Accuracy: 99.1%, Average Loss: 0.000483\n",
      "\n",
      "Epoch 4:\n",
      "loss:0.017270 [    0/60000]\n",
      "loss:0.000837 [ 6400/60000]\n",
      "loss:0.068242 [12800/60000]\n",
      "loss:0.013457 [19200/60000]\n",
      "loss:0.005260 [25600/60000]\n",
      "loss:0.004822 [32000/60000]\n",
      "loss:0.000804 [38400/60000]\n",
      "loss:0.018032 [44800/60000]\n",
      "loss:0.087563 [51200/60000]\n",
      "loss:0.032395 [57600/60000]\n",
      "Accuracy: 99.0%, Average Loss: 0.000593\n",
      "\n",
      "Epoch 5:\n",
      "loss:0.064512 [    0/60000]\n",
      "loss:0.000294 [ 6400/60000]\n",
      "loss:0.004837 [12800/60000]\n",
      "loss:0.018644 [19200/60000]\n",
      "loss:0.002140 [25600/60000]\n",
      "loss:0.007252 [32000/60000]\n",
      "loss:0.006488 [38400/60000]\n",
      "loss:0.001797 [44800/60000]\n",
      "loss:0.029557 [51200/60000]\n",
      "loss:0.018217 [57600/60000]\n",
      "Accuracy: 98.8%, Average Loss: 0.000569\n",
      "\n",
      "Epoch 6:\n",
      "loss:0.021394 [    0/60000]\n",
      "loss:0.001556 [ 6400/60000]\n",
      "loss:0.000986 [12800/60000]\n",
      "loss:0.022454 [19200/60000]\n",
      "loss:0.013808 [25600/60000]\n",
      "loss:0.001109 [32000/60000]\n",
      "loss:0.002163 [38400/60000]\n",
      "loss:0.010930 [44800/60000]\n",
      "loss:0.126944 [51200/60000]\n",
      "loss:0.009006 [57600/60000]\n",
      "Accuracy: 99.1%, Average Loss: 0.000565\n",
      "\n",
      "Epoch 7:\n",
      "loss:0.034413 [    0/60000]\n",
      "loss:0.000085 [ 6400/60000]\n",
      "loss:0.005945 [12800/60000]\n",
      "loss:0.054652 [19200/60000]\n",
      "loss:0.013326 [25600/60000]\n",
      "loss:0.003328 [32000/60000]\n",
      "loss:0.003338 [38400/60000]\n",
      "loss:0.002635 [44800/60000]\n",
      "loss:0.086092 [51200/60000]\n",
      "loss:0.000228 [57600/60000]\n",
      "Accuracy: 99.1%, Average Loss: 0.000575\n",
      "\n",
      "Epoch 8:\n",
      "loss:0.003117 [    0/60000]\n",
      "loss:0.000523 [ 6400/60000]\n",
      "loss:0.102316 [12800/60000]\n",
      "loss:0.005551 [19200/60000]\n",
      "loss:0.002842 [25600/60000]\n",
      "loss:0.007288 [32000/60000]\n",
      "loss:0.020533 [38400/60000]\n",
      "loss:0.004162 [44800/60000]\n",
      "loss:0.044668 [51200/60000]\n",
      "loss:0.000918 [57600/60000]\n",
      "Accuracy: 99.1%, Average Loss: 0.000652\n",
      "\n",
      "Epoch 9:\n",
      "loss:0.000506 [    0/60000]\n",
      "loss:0.000025 [ 6400/60000]\n",
      "loss:0.032855 [12800/60000]\n",
      "loss:0.000032 [19200/60000]\n",
      "loss:0.009233 [25600/60000]\n",
      "loss:0.009696 [32000/60000]\n",
      "loss:0.000253 [38400/60000]\n",
      "loss:0.009200 [44800/60000]\n",
      "loss:0.001995 [51200/60000]\n",
      "loss:0.001871 [57600/60000]\n",
      "Accuracy: 99.2%, Average Loss: 0.000596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#执行每个opoch\n",
    "for i in range(epoch):\n",
    "    print(f'Epoch {i}:')\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0005, Accuracy: 9899/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00052, 0.9899)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorbase] *",
   "language": "python",
   "name": "conda-env-tensorbase-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
