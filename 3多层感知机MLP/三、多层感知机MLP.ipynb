{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 神经网络和多层感知机MLP\n",
    "\n",
    "\n",
    "\n",
    "## 3.1. 感知机到神经网络，神经网络的基本结构\n",
    "\n",
    "### 3.1.1 感知机\n",
    "\n",
    "感知机模型是将若干输入加权求和并通过激活函数后输出的模型：\n",
    "\n",
    "![img](./imgs/图片1.png)\n",
    "\n",
    "感知机可以表示为线性变换+非线性激活函数：\n",
    "\n",
    "$z=∑_{i=1}^{m}w_ix_i+b$\n",
    "\n",
    "$sign(z) = \\left\\{ \\begin{aligned} -1　 　z & < &0 \\\\ 1　　  z & >= &0  \\end{aligned} \\right. $\n",
    "\n",
    "上述模型是一个二分类器，由于其过于简单，无法拟合复杂的非线性任务。\n",
    "\n",
    "### 3.1.2 神经网络及其基本结构\n",
    "\n",
    "神经网络则是基于这样的简单模型，将多个神经元逐层堆叠，由此形成我们的深度模型：\n",
    "\n",
    "![image-20210707094933084](./imgs/图片2.png)\n",
    "\n",
    "上图是一个最简单的神经网络，它包含神经网络的最基本结构：\n",
    "\n",
    "​\t输入层：对应输入向量的大小\n",
    "\n",
    "​\t输出层：对应模型任务，可以是二分类，多分类，也可以是回归任务等等\n",
    "\n",
    "​\t隐层：由多个感知机，即神经元组成，每个神经元对上层的所有输入做线性变换与非线性激活。\n",
    "\n",
    "### 3.1.3 神经网络参数的定义\n",
    "\n",
    "​\t神经网络的隐层可以不止一层，宽度也可以很宽，一般来说，我们计算网络层数时不考虑输入层，所以下图是一个４层的神经网络，由于每层神经元连接到上层的所有输入，这个网络也叫做全连接网络，或多层感知机：\n",
    "\n",
    "![img](./imgs/图片3.png)\n",
    "\n",
    "​\t由于网络中每层都有参数$w$和$b$，所以我们需要特定的方式进行定义：以下图一个三层的DNN为例，第二层的第4个神经元到第三层的第2个神经元的线性系数定义为$w^3_{24}$。上标3代表线性系数$w$所在的层数，而下标对应的是输出的第三层索引2和输入的第二层索引4。这样定义，每层进行的矩阵运算都可以表示为$w^Tx+b$。\n",
    "\n",
    "![img](./imgs/图片4.png)\n",
    "\n",
    "类似的偏置$b$，第二层的第三个神经元对应的偏倚定义为$b^2_3$。\n",
    "\n",
    "![img](./imgs/图片5.png)\n",
    "\n",
    "## 3.2. 前向传播\n",
    "\n",
    "### 3.2.1 前向传播原理\n",
    "\n",
    "上面讲到网络每层从输入到输出都可以表示为线性变换$z=w^Tx+b$与激活函数$\\sigma(z)$，前向传播顾名思义就是从输入层开始，逐层计算输出，最终得到输出层的输出后结束。\n",
    "\n",
    "<img src=\"./imgs/图片6.png\" alt=\"img\" style=\"zoom:60%;\" />\n",
    "\n",
    "​\t例如第二层的输出$a^2_1,a^2_2,a^2_3$，我们有：\n",
    "\n",
    "$a^2_1=σ(z^2_1)=σ(w^2_{11}x^1+w^2_{12}x^2+w^2_{13}x^3+b^2_1)$\n",
    "\n",
    "$a^2_2=σ(z^2_2)=σ(w^2_{21}x^1+w^2_{22}x^2+w^2_{23}x^3+b^2_2)$\n",
    "\n",
    "$a^2_3=σ(z^2_3)=σ(w^2_{31}x^1+w^2_{32}x^2+w^2_{33}x^3+b^2_3)$\n",
    "\n",
    "​\t对于第三层的的输出$a^3_1$，我们有：\n",
    "\n",
    "$a^3_1=σ(z^3_1)=σ(w^3_{11}x^1+w^3_{12}x^2+w^3_{13}x^3+b^3_1)$\n",
    "\n",
    "​\t将上面的例子一般化，假设第$l−1$层共有m个神经元，则对于第$l$层的第j个神经元的输出$a^l_j$，我们有：\n",
    "\n",
    "$a^l_j=σ(z^l_j)=σ(∑_{k=1}^mwl_{jk}a^{l−1}_k+b^l_j)$\n",
    "\n",
    "为了简化表示，假设$l-1$层共有m个神经元，$l$层共有n个神经元，第$l$层的$w$组成了一个$n\\times m$的矩阵$W^l$，那么第l层的输出可以表示为：\n",
    "\n",
    "$a^l=σ(z^l)=σ(W^la^{l−1}+b^l)$\n",
    "\n",
    "### 3.2.2 前向传播算法\n",
    "\n",
    "前向传播算法也就是利用若干个权重系数矩阵$W$,偏倚向量$b$来和输入值向量$x$进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直到运算到输出层，得到输出结果为值。\n",
    "\n",
    "　　　　输入: 总层数L，所有隐藏层和输出层对应的矩阵$W$,偏倚向量$b$，输入值向量$x$\n",
    "\n",
    "　　　　输出：输出层的输出$a^L$\n",
    "\n",
    "　　　　1） 初始化$a^1=x$\n",
    "\n",
    "　　　　2)  for $l=2$ to $L$, 计算：\n",
    "\n",
    "​\t\t\t\t\t\t$a^l=σ(z^l)=σ(W^la^{l−1}+b^l)$\n",
    "\n",
    "## 3.3. 反向传播\n",
    "\n",
    "​\t我们在定义了模型的结构后，神经网络的所有参数$W, b$都是随机的初始化状态，要想解决实际任务，我们必须令模型拟合我们的数据，也就是网络的训练过程。我们知道梯度下降法，它的意义是令参数向损失函数最大梯度的负方向移动，最终得到损失函数的局部最小，以此来拟合数据，而反向传播算法就是基于梯度下降，因为损失函数由最后一层的输出计算得到，所以梯度需要从最深层向最浅层传播，逐层计算梯度，以此来更新每层的参数。\n",
    "\n",
    "### 3.3.1 反向传播基本概念\n",
    "\n",
    "​\t反向传播的第一步是正向传播，计算出网络每层的输出：$a^l=σ(z^l)=σ(W^la^{l−1}+b^l)$，得到最后一层输出$a^L$后，计算损失函数$J$，常用的损失函数有平方损失，交叉熵损失等\n",
    "\n",
    "![image-20210707162616481](./imgs/图片7.png)\n",
    "\n",
    "​\t为了方便讲解，我们使用平方损失，对于每个样本，最小化下式：\n",
    "\n",
    "$J(W,b,x,y)=\\frac{1}{2}||a^L−y||^2$\n",
    "\n",
    "​\t对于输出层$a^L=σ(z^L)=σ(W^La^{L−1}+b^L)$，损失函数为$J(W,b,x,y)=\\frac{1}{2}||a^L−y||^2=\\frac{1}{2}||σ(W^La^{L−1}+b^L)−y||^2$\n",
    "\n",
    "那么损失函数对于$W$和$b$的梯度可以按下式计算：\n",
    "\n",
    "$\\frac{∂J(W,b,x,y)}{∂W^L}=[(a^L−y)⊙σ′(z^L)](a^{L−1})^T$\n",
    "\n",
    "$\\frac{∂J(W,b,x,y)}{∂b^L}=(a^L−y)⊙σ′(z^L)$\n",
    "\n",
    "$⊙$代表Hadamard积对于两个维度相同的向量$A（a1,a2,...an）^T$和$B（b1,b2,...bn）^T$,则$A⊙B=(a1b1,a2b2,...anbn)^T$。\n",
    "\n",
    "$W,b$梯度的公共部分，也就是损失对$z^L$的梯度记为：\n",
    "\n",
    "$δ^L=\\frac {∂J(W,b,x,y)}{∂z^L}=(a^L−y)⊙σ′(z^L)$\n",
    "\n",
    "得到了第L层的梯度后，继续计算$L-1$层及之前层的梯度，根据链式法则，前面任意一层$z^l$的梯度可以表示为：\n",
    "\n",
    "$δ^l=\\frac{∂J(W,b,x,y)}{∂z^l}=(\\frac{∂z^L}{∂z^{L−1}}\\frac{∂z^{L−1}}{∂z^{L−2}}...\\frac{∂z^{l+1}}{∂z^l})^T\\frac{∂J(W,b,x,y)}{∂z^L}$\n",
    "\n",
    "当得到了深$l$层$z^L$的梯度后，很容易可以求得$l-1$层$W,  b$的梯度：\n",
    "\n",
    "$\\frac{∂J(W,b,x,y)}{∂W^l}=δ^l(a^{l−1})^T$\n",
    "\n",
    "$\\frac{∂J(W,b,x,y)}{∂b^l}=δ^l$\n",
    "\n",
    "假设$δ^{L+1}$已经得到，现在求取$δ^{L}$：\n",
    "\n",
    "$δl=\\frac{∂J(W,b,x,y)}{∂z^l}=(\\frac{∂z^{l+1}}{∂z^l})^T\\frac{∂J(W,b,x,y)}{∂z^{l+1}}=(\\frac{∂z^{l+1}}{∂z^l})^Tδ^{l+1}$\n",
    "\n",
    "因为：\n",
    "\n",
    "$z^{l+1}=W^{l+1}a^l+b^{l+1}=W^{l+1}σ(z^l)+b^{l+1}$\n",
    "\n",
    "得到：\n",
    "\n",
    "$\\frac {∂z^{l+1}}{∂z^l}=W^{l+1} diag(σ′(z^l))$\n",
    "\n",
    "代入得：\n",
    "\n",
    "$δl=(\\frac{∂z^{l+1}}{∂z^l})^Tδ^{l+1}=diag(σ′(z^l))(W^{l+1})^Tδ^{l+1}=(W^{l+1})^Tδ^{l+1}⊙σ′(z^l)$\n",
    "\n",
    "### 3.3.2 反向传播算法\n",
    "\n",
    "​\t　输入: 总层数$L$，以及各隐藏层与输出层的神经元个数，损失函数，学习率α，epoch数MAX，输入的m个训练样本${(x1,y1),(x2,y2),...,(xm,ym)}{(x1,y1),(x2,y2),...,(xm,ym)}$\n",
    "\n",
    "​\t1) 初始化各隐藏层与输出层的线性关系系数矩阵$W$和偏倚向量$b$的值为一个随机值。\n",
    "\n",
    " \t2）for iter to 1 to MAX：\n",
    "\n",
    "​\t\t2-1) for i =1 to m：\n",
    "\n",
    "　　\ta) 将DNN输入$a^1$设置为$x^i$\n",
    "\n",
    "　　　b) for $l=2$ to $L$，进行前向传播算法计算\t$a^{i,l}=σ(z^i,l)=σ(W^la^{i,l−1}+b^l)a^{i,l}=σ(z^{i,l})=σ(W^la^{i,l−1}+b^l)$\n",
    "\n",
    "　　　c) 通过损失函数计算输出层的$δ^{i,L}$\n",
    "\n",
    "　　　d) for $ l= L-1$ to 2, 进行反向传播算法计算$δ^{i,l}=(W^{l+1})^Tδ^{i,l+1}⊙σ′(z^{i,l})$\n",
    "\n",
    "　　2-2) for $l = 2$ to $L$，更新第$l$层的$W^l, b^l$:\n",
    "\n",
    "​\t\t\t$W^l=W^l−α∑_{i=1}^{m}δ^{i,l}(a^{i,l−1})^T$\n",
    "\n",
    "​\t\t\t$b^l=b^l−α∑_{i=1}^{m}δ^{i,l}$\n",
    "\n",
    "## 3.4. 激活函数\n",
    "\n",
    "​\t如果神经网络没有非线性的激活函数，那么网络中就只存在线性变换，网络就无法拟合复杂的非线性问题。\n",
    "\n",
    "### 3.4.1 Sigmoid\n",
    "\n",
    "Sigmoid函数：\n",
    "\n",
    "$\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "Sigmoid函数的导数：\n",
    "\n",
    "$\\sigma'(z)=(1-\\sigma(z))\\sigma(z)$\n",
    "\n",
    "函数图像：\n",
    "\n",
    "<img src=\"./imgs/图片8.png\" alt=\"image-20210708093403075\" style=\"zoom:30%;\" />\n",
    "\n",
    "优点：输出在01之间，适应于特定任务，也可以将数值很大的输入进行缩放；平滑，任意处可导。\n",
    "\n",
    "缺点：\n",
    "\n",
    "​\t1. 在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大，如下图导数图像，当输入很大或很小时，梯度趋近于0：\n",
    "\n",
    "<img src=\"./imgs/图片9.png\" alt=\"image-20210708093602513\" style=\"zoom:50%;\" />\n",
    "\n",
    " \t2. 输出不是0均值，这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入，随着网络的加深，会改变数据的原始分布。\n",
    " \t3. 计算量大。\n",
    "\n",
    "### 3.4.2 tanh\n",
    "\n",
    "tanh函数：\n",
    "\n",
    "$tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "\n",
    "tanh导数：\n",
    "\n",
    "$tanh'(x)=\\frac{2}{1+e^{-2x}}-1$\n",
    "\n",
    "函数曲线：\n",
    "\n",
    "<img src=\"./imgs/图片10.png\" alt=\"image-20210708094019481\" style=\"zoom:60%;\" />\n",
    "\n",
    "导数曲线：\n",
    "\n",
    "<img src=\"./imgs/图片11.png\" alt=\"image-20210708094039328\" style=\"zoom:60%;\" />\n",
    "\n",
    "优点：\n",
    "\n",
    " \t1. 范围在-1,1之间，解决了非0均值的问题；\n",
    "\n",
    "缺点：\n",
    "\n",
    " \t1. 计算量大；\n",
    " \t2. 梯度消失、梯度爆炸。\n",
    "\n",
    "### 3.4.3 ReLU\n",
    "\n",
    "ReLU函数：\n",
    "\n",
    "$ReLU(x) = max(0,x)$\n",
    "\n",
    "函数曲线：\n",
    "\n",
    "<img src=\"./imgs/图片12.png\" alt=\"image-20210708094617383\" style=\"zoom:60%;\" />\n",
    "\n",
    "导数曲线：\n",
    "\n",
    "<img src=\"./imgs/图片13.png\" alt=\"image-20210708094635974\" style=\"zoom:67%;\" />\n",
    "\n",
    "优点：\n",
    "\n",
    " \t1. 正区间内导数都为1，缓解了梯度消失的问题；\n",
    " \t2. 计算速度快；\n",
    " \t3. 加快网络收敛。\n",
    "\n",
    "缺点：\n",
    "\n",
    " \t1. 不是0均值；\n",
    " \t2. Dead ReLU Problem：某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. 使用Pytorch框架构建MLP \n",
    "环境：Pytorch  \n",
    "设备：GPU  \n",
    "数据集：MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 导入Pytorch包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# 选择gpu或cpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 加载MNIST数据集，创建训练集和测试集的DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "training_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAckAAAHRCAYAAAABukKHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nO3de5RU1Zn38d+DIBeVi2IUjQIjMF6IEERHlBeM4g0R0CwiiiIYLyNLFBMYvI3BQGLAaBaDo2hUlMEJMVHAGwGiSPCWkDg4QW2jGAgICCrYCASinPePrkx6n312d1FU16nL97NWrdXP00/V2dVu+nH32XWORVEkAADga5T2AAAAKFY0SQAAAmiSAAAE0CQBAAigSQIAEECTBAAgoOyapJmtMrN+WdRFZtYpx2Pk/FyUH+Yc0sC8K4yya5JpsxqTzeyTzGOKmVna40L5M7N9zazKzNamPRaUPzP7hpktNrPPzGxV2uNpKDTJ/Lta0mBJ3SQdL2mApGtSHREqxThJG9MeBCrGNkmPqGbela2ybZJmdpKZvWZmW8xsvZnda2b7xsr6m9kHZvaxmd1lZo1qPf8KM3vHzDab2QIza5/loS+XdHcURWujKPpQ0t2SRuTnXaGYpTjnZGYdJV0q6c48vR2UiLTmXRRFv4ui6L8kfZDP91NsyrZJSvpS0o2S2krqJekMSaNiNRdI6imph6RBkq6QJDMbLOkWSRdKOljSUkk/SzqImV1iZv9bK3WcpDdrxW9mcih/ac05SZqWef6OfLwRlJQ05135i6KorB6SVknql5AfI2lOrTiSdE6teJSkFzJfz5f07VrfayRpu6T2tZ7bKXD8LyUdXSvunKm3tH82PMp2zl0g6VeZr0+TtDbtnwmPhn+kPe9qPaefpFVp/zwa6lG2K0kz62Jmz5rZBjOrlvRD1fyfVm1ran29WtJhma/bS5qa+fPFFkmfSjJJh2dx6M8ltawVt5T0eZSZTShfacw5M9tP0hRJo/PxHlB6UvxdVxHKtklKul9SlaTOURS1VM2fFOK7TI+o9fWRktZlvl4j6ZooilrXejSPoujVLI77lmo27fxdt0wO5S+NOddZUgdJS81sg6SnJLXL/MLssFfvBqUird91FaGcm+QBkqolfW5mR0u6NqFmnJm1MbMjJN0g6eeZ/HRJN5vZcZJkZq3MbEiWx50p6TtmdriZHSbpu5Ie3Yv3gdKRxpxboZpfgN0zjyslfZT5ek0dz0P5SOV3nZk1MrNmkprUhNYsYcNQySvnJjlW0iWStkr6qf4xKWqbJ+kPkpZLek7Sw5IURdEcSZMlzc78+WKFpHOTDmJmw8ys9krxAUnPSPpj5nnPZXIofwWfc1EUfRFF0Ya/P1Tz57LdmfjLvL47FKu0ftf1Uc1GsedVszrdIWlhHt5PUTFOlQEAkKycV5IAAOwVmiQAAAE0SQAAAmiSAAAE0CQBAAhoXNc3zYytrxUsiqJUbvHFvKtsacw75lxlq2vOsZIEACCAJgkAQABNEgCAAJokAAABNEkAAAJokgAABNAkAQAIoEkCABBAkwQAIIAmCQBAAE0SAIAAmiQAAAE0SQAAAmiSAAAE0CQBAAigSQIAEECTBAAgoHHaAyg1J5xwghNfd911Xs3w4cOdeObMmV7NtGnTnPiNN97Iw+gAAPnEShIAgACaJAAAATRJAAACaJIAAARYFEXhb5qFv1kBunfv7uVefPFFJ27ZsmVOr/3ZZ5858UEHHZTT6zSkKIosjeNW+rwrpDPOOMPLPf74416ub9++Tvzuu+822JjSmHfMufy47bbbvNwdd9zhxI0a+Wuz0047zcstWbIkb+OqT11zjpUkAAABNEkAAAJokgAABHAxgYyTTjrJyz355JNerlWrVk6cdE5369atTrxr1y6vJn4O8uSTT/Zqki4wkPRaSNanTx8vF/+5z5kzp1DDKUonnniil1u2bFkKI0EpGjFihBOPHz/eq9m9e3e9r1PX3pi0sZIEACCAJgkAQABNEgCAAJokAAABFbFxp0WLFl6uR48eTjxr1iyvpl27djkd77333nPiKVOmeDWzZ8924ldeecWrSfpg7p133pnTmCpR0geUO3fu7MSVtnEn/kHujh07ejXt27f3cmapXFcCRS4+V5o1a5bSSBoOK0kAAAJokgAABNAkAQAIoEkCABBQERt3HnjgAS938cUXN9jx4puC9t9/f68mfoX7pE0mxx9/fF7HVWmGDx/u5V577bUURlI84pvRrrrqKq8maRNbVVVVg40JpaFfv35ebvTo0fU+Lz53BgwY4NV89NFHuQ+sgbGSBAAggCYJAEAATRIAgICyPCd5wgknOPF5553n1WTz4eikO2M/88wzTvzjH//Yq1m3bp0T/8///I9Xs3nzZic+/fTTcxojwpLugF7pHnrooXpr4hfDQGXq3bu3E8+YMcOrid8VKcldd93lxKtXr967gRUYv0UAAAigSQIAEECTBAAggCYJAEBAyW/c6d69u5dbtGiRE7ds2dKriaLIiefPn+/VJF1woG/fvk6cdKeO+OaITZs2eTVvvvmmE+/evdurSdpwFL9QwRtvvOHVVKr4xRcOOeSQlEZSvLLZaBH/94PKdPnllzvxYYcdVu9zXnrpJS83c+bMfA0pFawkAQAIoEkCABBAkwQAIKDkzkl26dLFiceNG+fVxM+7fPzxx17N+vXrnfixxx7zaj7//HMv99xzz9UZ51Pz5s293He/+10nHjZsWIMdv9T079/fiZN+fpUk6Zxsx44d633ehx9+2BDDQRFr27atl7viiiucOGnfxJYtW5x40qRJ+R1YEWAlCQBAAE0SAIAAmiQAAAE0SQAAAop6407Tpk29XPyuG/HNGpK0detWJ066Q/3vf/97Jy6VTR5HHnlk2kMoWv/8z/9cb81bb71VgJEUh6Q71MQ38/zpT3/yauL/flBeOnTo4OWefPLJnF5r2rRpTrx48eKcXqeYsZIEACCAJgkAQABNEgCAgKI+J/n1r3/dyyWdg4wbNGiQEy9ZsiRvY0JpW7ZsWdpDyEn8Iv3nnHOOV3PppZc68VlnnVXv606cONHLxT8gjvKSNHfiNwdI8sILL3i5qVOn5mVMxYyVJAAAATRJAAACaJIAAATQJAEACCjqjTv33HOPlzMzJ07alFOKG3UaNfL/fyXpqvvYOwceeGBeXqdbt25eLj43+/Xr59V89atfdeJ9993Xq0m6s0t8fuzYscOr+e1vf+vEO3fu9GoaN3b/yf/hD3/walBeBg8e7MQ/+tGPsnreyy+/7MSXX365V/PZZ5/lPrASwUoSAIAAmiQAAAE0SQAAAmiSAAAEFM3GnQEDBni57t27e7koipz46aefbrAxFVLSJp34e5Wk5cuXF2I4JSm+mSXp5zd9+nQnvuWWW3I6VtIVSuIbd7744guvZvv27U789ttvezWPPPKIl4vftSZpc9pHH33kxGvXrvVq4ne7qaqq8mpQuvJ5h48PPvjAiePzq1KwkgQAIIAmCQBAAE0SAICAojknGT9XIiV/0Hrjxo1O/POf/7zBxpQvTZs29XITJkyo93kvvviil7v55pvzMaSyNGrUKCdevXq1V3PKKafk5Vh/+ctfvNzcuXOd+J133vFqXn/99bwcP8nVV1/txAcffLBXEz/PhPIyfvx4L5frRUmyvehAuWMlCQBAAE0SAIAAmiQAAAE0SQAAAopm40624nc2WL9+fUojCYtv1Lntttu8mnHjxjlx0ge/7777bi/3+eef7+XoKsfkyZPTHkJBnXHGGfXW5PrBchSn+AVXzjrrrJxeZ968eV7u3Xffzem1yg0rSQAAAmiSAAAE0CQBAAgouXOSxXZB86SLsMfPN1500UVeTfwcwDe/+c38DgxIMGfOnLSHgDxauHChE7dp06be5yRd0GLEiBH5GlLZYSUJAEAATRIAgACaJAAAATRJAAACimbjTvyu7qHc4MGDnfiGG25osDElufHGG5343//9372aVq1aOfHjjz/u1QwfPjy/AwNQcQ466CAnzuaOH/fdd5+X4yIlYawkAQAIoEkCABBAkwQAIKBozklGUZRV7tBDD3Xi//iP//BqHnnkESf+5JNPvJqTTz7ZiS+77DKvplu3bl7uq1/9qhMn3aF+wYIFTpx0DgBoaEnn9Lt06eLESR8sR3GaMWOGl2vUaM/XOa+++mo+hlMxWEkCABBAkwQAIIAmCQBAAE0SAICAotm4k6199tnHiUeNGuXVxO+oUV1d7dV07tw5p+PHT3ovXrzYq7n99ttzem0gn5I2vuWy0QPpiN9hqF+/fl5N/OIBu3bt8mr+8z//04k/+uijPIyucvAvBgCAAJokAAABNEkAAAJokgAABBTNxp3XXnvNyy1btszLnXjiifW+VvyqPIcccki9z0m6Ks/s2bO9XKHvOgLkU69evZz40UcfTWcgqFfr1q2dOP57LcmHH37o5caOHZu3MVUiVpIAAATQJAEACKBJAgAQUDTnJNeuXevlLrzwQi93zTXXOPFtt92W0/GmTp3qxPfff79X8/777+f02kAxSLoLCIA9w0oSAIAAmiQAAAE0SQAAAmiSAAAEFM3GnSTr16/3chMmTKgzBirV/PnznXjIkCEpjQT5UFVV5cTxOxBJUu/evQs1nIrFShIAgACaJAAAATRJAAACLOnu5f/3TbPwN1H2oihK5dPozLvKlsa8Y85VtrrmHCtJAAACaJIAAATQJAEACKBJAgAQQJMEACCAJgkAQABNEgCAAJokAAABNEkAAAJokgAABNAkAQAIoEkCABBAkwQAIKDOu4AAAFDJWEkCABBAkwQAIIAmCQBAAE0SAIAAmiQAAAE0SQAAAmiSAAAE0CQBAAigSQIAEFB2TdLMVplZvyzqIjPrlOMxcn4uyg9zDmlg3hVG2TXJtJlZazN7zMw2Zh4T0h4TypuZjTOzFWa21cz+bGbj0h4Typ+ZfcPMFpvZZ2a2Ku3xNBSaZP79RFILSR0knSTpMjMbmeqIUO5M0nBJbSSdI+k6Mxua7pBQAbZJekRSWf9PWdk2STM7ycxeM7MtZrbezO41s31jZf3N7AMz+9jM7jKzRrWef4WZvWNmm81sgZm1z/LQ50uaEkXR9iiKVkl6WNIV+XlXKGZpzbkoiqZEUfRGFEVfRFH0rqR5kk7N41tDEUtx3v0uiqL/kvRBPt9PsSnbJinpS0k3SmorqZekMySNitVcIKmnpB6SBinTzMxssKRbJF0o6WBJSyX9LOkgZnaJmf1vPB37uuvevBGUjDTn3N+/Z5L+n6S39vK9oHSkPu/KWhRFZfWQtEpSv4T8GElzasWRpHNqxaMkvZD5er6kb9f6XiNJ2yW1r/XcToHjz5L0lKQDJHWStFLSzrR/LjzKd87FjnmHpDclNU3758KjYR/FMu8k9ZO0Ku2fR0M9ynYlaWZdzOxZM9tgZtWSfqia/9OqbU2tr1dLOizzdXtJUzN/vtgi6VPVrAgPz+LQ10vaIek91fzZ62eS1ub+TlAqUpxzfz/+dao5N3leFEU7c30fKC1pz7tyV7ZNUtL9kqokdY6iqKVq/qRgsZojan19pKR1ma/XSLomiqLWtR7Noyh6tb6DRlH0aRRFw6IoOjSKouNU8zP+3V6/G5SCVOacVHNeSdJNks6Iooj/Kassqc27SlDOTfIASdWSPjezoyVdm1AzzszamNkRkm6Q9PNMfrqkm83sOEkys1ZmNiSbg5rZUWZ2kJntY2bnSrpa0qS9fTMoCWnNuWGqWT2cGUVRWW+iQKK05l0jM2smqUlNaM0SNgyVvHJukmMlXSJpq6Sf6h+TorZ5kv4gabmk51SzE1VRFM2RNFnS7MyfL1ZIOjfpIGY2zMxqb5I4QdIfM8e9U9KwKIrYRFEZ0ppzkyQdJGmZmX2eeUzPz1tCCUhr3vVRzaml51WzOt0haWEe3k9RscyJVwAAEFPOK0kAAPYKTRIAgACaJAAAATRJAAACGtf1TTNjV08Fi6Io/lmrgmDeVbY05h1zrrLVNedYSQIAEECTBAAggCYJAEAATRIAgACaJAAAATRJAAACaJIAAATQJAEACKBJAgAQQJMEACCAJgkAQABNEgCAAJokAAABNEkAAAJokgAABNAkAQAIoEkCABDQOO0BAAAaztSpU534+uuv92pWrFjh5QYMGODEq1evzu/ASgQrSQAAAmiSAAAE0CQBAAigSQIAEMDGHaAEHHDAAU68//77ezXnnXeeEx988MFezT333OPEO3fuzMPoUCw6dOjg5S699FIn3r17t1dzzDHHeLmjjz7aidm4AwAAHDRJAAACaJIAAARwThJIUdI5pPHjx3u5Xr16OXHXrl1zOl67du2cOOmD5ShdmzZt8nK/+c1vnHjgwIGFGk5ZYCUJAEAATRIAgACaJAAAATRJAAACKnbjzr/8y784cfwDt5LUt29fL3fcccfV+9pjx4514nXr1nk1vXv3duJZs2Z5Nb/97W/rPRaKV/zD2JI0ZswYJx42bJhX07x5cy9nZk68Zs0ar2br1q1OnPQB8W9961tOfN9993k1VVVVXg6lYdu2bV6uUi8CkC+sJAEACKBJAgAQQJMEACCAJgkAQEBFbNy56KKLvNzUqVOduG3btl5NfLOEJL300ktOnHSnhbvuuqveMcVfO+l1hg4dWu/rIB2tWrXycpMnT3bipHkXv5tHtt577z0nPvvss72aJk2aOHHSBpz4PE+a9yhdrVu39nLdunVLYSTlg5UkAAABNEkAAAJokgAABJT8OcnGjf230LNnTyf+6U9/6tW0aNHCieNXypekiRMnermXX37ZiZs2berVPPHEE0581llneTVxv//97+utQfG44IILvNyVV16Zl9deuXKllzvzzDOdOOliAp06dcrL8VG64r/XJOnII4/M6bVOPPFEJ046x10JFypgJQkAQABNEgCAAJokAAABNEkAAAJKfuNO0t07HnrooXqft2jRIidO+uB3dXV1va+T9LxsNuqsXbvWiR977LF6n4PiMWTIkJyet2rVKidetmyZVzN+/Hgvl7RRJy7prh+oLEl3HHr00UedeMKECVm9Vrxuy5YtXs29996b7dBKFitJAAACaJIAAATQJAEACCi5c5LxD/jfcsstXk0URU6cdPf12267zYmzOf+Y5NZbb83peddff70Tb9q0KafXQTquuuoqL3f11Vc78cKFC72a999/34k3btyYtzEdcsgheXstlI/478xsz0miBitJAAACaJIAAATQJAEACKBJAgAQUNQbd26//XYvF9+os2vXLq9mwYIFTpz04ewdO3bUe/xmzZp5ufiFApKusG9mTjxp0iSvZt68efUeH8Ur6UPbaW+I6NWrV6rHR2lo1MhfG+3evTuFkZQGVpIAAATQJAEACKBJAgAQUDTnJFu3bu3lRo0a5eXiFwqIn3+UpMGDB+/x8ZPu6v744497uRNOOKHe1/rlL3/pxFOmTNnj8aAyxC8qIUn77bdfTq/1ta99rd6aV1991Ylfe+21nI6F0pV0/jH+exX/wEoSAIAAmiQAAAE0SQAAAmiSAAAEFM3GnX333dfLtW3btt7nJW18+MpXvuLEI0eO9GoGDhzoxF27dvVq9t9/fy8XP8GddMJ71qxZTrxt2zavBuWnRYsWTnzsscd6Nd/73vecuH///lm9dvwD4Nl8+DvpggfxfwtffvllVscHKhUrSQAAAmiSAAAE0CQBAAigSQIAEFA0G3eS7uaxadMmL3fwwQc78Z///GevJperRyRtcqiurvZy7dq1c+KPP/7Yq3nmmWf2+Pgobk2aNHHir3/9617Nk08+6cTxuSL5d59JmndJV8E555xznDi+SShJ48b+P+8LL7zQiadOnerVJP1bBCoVK0kAAAJokgAABNAkAQAIKJpzklu2bPFySXfzePbZZ534wAMP9GpWrlzpxPPmzfNqHn30USf+9NNPvZrZs2d7ufh5pqQalLakC1vEzwk+9dRT9b7OHXfc4eVefPFFJ37llVe8mqQ5HX9e0sUv4uLn7yXpzjvvdOK//OUvXs3cuXO93M6dO+s9HkpD/MIUUnYXp+jTp4+Xu/fee/MypmLGShIAgACaJAAAATRJAAACaJIAAARYXR+8N7M9/1R+iUo6Kb1kyRIvFz/BPWbMGK9m2rRp+RtYiqIosjSOW8h5F79IgCR9//vf93Ljxo2r97Xmz5/vxJdddplXE9+glrS55vnnn/dyPXr0cOKkD/xPmTLFiZM29wwaNMjLxf3617/2cpMnT3bizZs31/s6y5cvr7cmSRrzrpJ+1yXd+SWXC7BI0vHHH+/Eb7/9dk6vk7a65hwrSQAAAmiSAAAE0CQBAAgomosJpK158+ZeLukDtvG/3XMxgdKyzz77OPHEiRO9mrFjx3q5bdu2OfFNN93k1cTnQtIFMnr27OnESR/GTrp4+nvvvefE1157rVezePFiJ27ZsqVXc8oppzjxsGHDvJqBAwd6uUWLFnm5uDVr1jhxx44d630OCm/69Ole7pprrsnpta6++monTtqjUepYSQIAEECTBAAggCYJAEAATRIAgAA27mQsWLAg7SGgAOIbDZI26Wzfvt3LxTc2LFy40Ks5+eSTnXjkyJFezbnnnuvESRvGki5mMGPGDCeOb5JJUl1d7eV+9atf1RlL0sUXX+zlLrnkknqPd+ONN9Zbg/RVVVWlPYSSwkoSAIAAmiQAAAE0SQAAArjAecbZZ5/t5ZIuNB3/ebVr186r2bRpU/4GlqJyvMD5+vXrnTjpAuM7d+70cvHzOPvtt59X06lTpz0ez4QJE7zcnXfe6eWSLkpdrrjAeeH96U9/8nJHHXVUvc9r1MhdZyX9G1i5cmXuAysQLnAOAEAOaJIAAATQJAEACKBJAgAQwMUEMv7pn/4p7SGgADZs2ODESRt3mjZt6uW6detW72vHN3r95je/8Wrmzp3rxKtWrfJqKmmTDorDW2+95eWy+Z2YdKekcsNKEgCAAJokAAABNEkAAAJokgAABLBxJ2Pp0qVeLn41CakyTlSXsz59+jjx4MGDvZoePXp4uY0bNzrxI4884tVs3rzZiXft2pXLEIGCe/DBB73c+eefn8JIig8rSQAAAmiSAAAE0CQBAAjgLiB1SLoyfvwDtr179/ZqXn/99QYbUyGV411AUPy4C0jhtW/f3ss9++yzTnzMMcd4NWbuf6ouXbp4NdwFBACAMkWTBAAggCYJAEAATRIAgAA27tRhxIgRXu6hhx5y4iVLlng1o0ePduK33347r+MqFDbuIA1s3EGhsXEHAIAc0CQBAAigSQIAEMA5yTq0bNnSyz3xxBNO3K9fP6/mqaeecuKRI0d6Ndu2bdvL0TU8zkkiDZyTRKFxThIAgBzQJAEACKBJAgAQQJMEACCAjTt7KL6Z5wc/+IFXc+211zrx8ccf79WUwgUG2LiDNLBxB4XGxh0AAHJAkwQAIIAmCQBAAOckEcQ5SaSBc5IoNM5JAgCQA5okAAABNEkAAAJokgAABNS5cQcAgErGShIAgACaJAAAATRJAAACaJIAAATQJAEACKBJAgAQQJMEACCAJgkAQABNEgCAgLJrkma2ysz6ZVEXmVmnHI+R83NRfphzSAPzrjDKrkmmzczGmNkHZlZtZuvM7Cdm1jjtcaF8mdk3zGyxmX1mZqvSHg8qi5nta2ZVZrY27bE0BJpk/j0jqUcURS0ldZXUTdL16Q4JZW6bpEckjUt7IKhI4yRtTHsQDaVsm6SZnWRmr5nZFjNbb2b3mtm+sbL+mVXfx2Z2l5k1qvX8K8zsHTPbbGYLzKx9NseNomhlFEVb/v4yknZLqug/V1SKFOfc76Io+i9JH+Tz/aA0pDXvMs/tKOlSSXfm6e0UnbJtkpK+lHSjpLaSekk6Q9KoWM0FknpK6iFpkKQrJMnMBku6RdKFkg6WtFTSz5IOYmaXmNn/JuSqJX2smpXkA/l5Syhyqc05VLQ05920zPN35OONFKUoisrqIWmVpH4J+TGS5tSKI0nn1IpHSXoh8/V8Sd+u9b1GkrZLal/ruZ2yGEtnSRMlHZr2z4VHwz2KZc5J6idpVdo/Dx6FeaQ971TTeH+V+fo0SWvT/pk0xKNsV5Jm1sXMnjWzDZlV3Q9V839ata2p9fVqSYdlvm4vaWrmzxdbJH2qmj+dHr4nY4ii6D1Jb0m6L5f3gNJSDHMOlSeNeWdm+0maIml0Pt5DMSvbJinpfklVkjpHNZtoblHNf/zajqj19ZGS1mW+XiPpmiiKWtd6NI+i6NUcxtFY0lE5PA+lp1jmHCpLGvOus6QOkpaa2QZJT0lql2nUHfbq3RSZcm6SB0iqlvS5mR0t6dqEmnFm1sbMjpB0g6SfZ/LTJd1sZsdJkpm1MrMh2RzUzK40s69kvj5W0s2SXti7t4ISkdaca2RmzSQ1qQmtWcLGDZSvNObdCtU03u6Zx5WSPsp8vaaO55Wccm6SYyVdImmrpJ/qH5OitnmS/iBpuaTnJD0sSVEUzZE0WdLszJ8vVkg6N+kgZjbMzN6qlTpV0h/NbJuk5zOPW/LxhlD00ppzfVSzceJ51awSdkhamIf3g9JQ8HkXRdEXURRt+PtDNX+m3Z2Jv8zru0uZZU66AgCAmHJeSQIAsFdokgAABNAkAQAIoEkCABBAkwQAIKDOWziZGVtfK1gURfEPJBcE866ypTHvmHOVra45x0oSAIAAmiQAAAE0SQAAAmiSAAAE0CQBAAigSQIAEECTBAAggCYJAEAATRIAgACaJAAAATRJAAACaJIAAATQJAEACKBJAgAQQJMEACCAJgkAQABNEgCAgMZpDwAAgLgXXnjBic3Mqzn99NMbfBysJAEACKBJAgAQQJMEACCAJgkAQEBFbNzp0qWLl2vSpIkT9+nTx6u57777vNzu3bvzN7Ba5s2b5+WGDh3q5Xbt2tUgx0dhxOfdKaec4tX88Ic/9HKnnnpqg40JSNtPfvITLxf/tzFz5sxCDcfBShIAgACaJAAAATRJAAACSv6c5HHHHeflRowY4cRDhgzxaho1cv//4LDDDvNqks4/RlG0hyPMzsCBA73c9OnTvdyYMWOcuLaOrGAAAAjgSURBVLq6ukHGg4bRqlUrJ168eLFXs2HDBi936KGH1lsDlIIf/ehHXu5f//Vfvdzf/vY3J45fXKBQWEkCABBAkwQAIIAmCQBAAE0SAIAAq2sjipk1zC6VPHr66ae9XP/+/fPy2klXnW+ojTvZ6tu3rxO/8sorDXasKIr8H0ABlMK8y1Xbtm2deOPGjVk9r0ePHk68fPnyvI2p2KQx78p5zhWbl156ycv17t3by8U3tZ155pkNNaQ65xwrSQAAAmiSAAAE0CQBAAigSQIAEFDyV9xZtGiRl8tm4058w8TDDz/s1cSvyiNldxeQ+NXr45ttgLokbRgDchW/w9Gtt97q1Vx88cVe7tNPP83L8eOv3bVrV69m5cqVXm7s2LF5Of7eYiUJAEAATRIAgACaJAAAASV/MYHGjf3Tqu3atav3efErzOfzrgotW7Z04hUrVng1SXcdiZs7d66XGzZsmBPv3LlzD0eXPS4mkH+5Xkwgfp779ddfz9uYig0XE8ivqqoqJ+7cubNXk7Rv4uWXX87L8f/4xz86cdI5yQsvvNDLzZkzJy/HzwYXEwAAIAc0SQAAAmiSAAAE0CQBAAgo+YsJfPHFF15uzZo1KYzkH84++2wnbtOmTU6vs3btWi/XkBt1ULx69uzpxOW8cQf5tX37didO2qzZrFmzvByre/fuXq59+/ZOnHRBlnwdvyGwkgQAIIAmCQBAAE0SAICAkj8nmbahQ4d6uauuusqJmzdvntNr33777Tk9D8Urfg79s88+82patWrl5Y466qgGGxPKx8SJE73c1772NSd+5513vJo333wzp+Ptt99+Tjx+/HivpkWLFk6cdD79l7/8ZU7HLwRWkgAABNAkAQAIoEkCABBAkwQAIICNO3WI33FDkm666SYn7tSpk1fTpEmTPT7W8uXLvVz8TiUofVu2bHHipUuXejUDBgwo1HBQ4o444ggnjm8alPzNYtddd51Xs2nTppyOf8899zjxkCFDvJp169Y58amnnprTsdLCShIAgACaJAAAATRJAAACSv6cZIcOHbzcZZdd5sT9+vXL6bV79+7t5ZIuDlyf6upqLxc/t/n88897NTt27NjjYwEoT127dvVyc+bMceK2bdt6NdOmTXPiJUuW5HT8sWPHerkRI0bU+7wf/OAHOR2vWLCSBAAggCYJAEAATRIAgACaJAAAASW3cSd+8vrpp5/2ao488shCDScrSR8Yf/DBB1MYCUrVQQcdlPYQ0IAaN/Z/FV966aVO/PDDD3s1jRq565zdu3d7Nb169XLim2++2auJXxRAkg488EAnTrpQgJk58cyZM72aBx54wMuVElaSAAAE0CQBAAigSQIAEECTBAAgoOQ27sTFTxyHcrmInxSXkk+M1yfprg7nnnuuE8+fP3+PXxeVY+DAgWkPAQ1o6NChXu6hhx5y4qSrfcV/H73//vteTc+ePeuMJWnQoEFe7vDDD3fidu3aeTXxu4dcccUVXk2pYyUJAEAATRIAgACaJAAAASV3TnLFihVOfNppp3k18Q/hLliwwKv561//mpfxfPvb3/Zyo0ePzstro/wtXrzYyyWdw0b5uOiii7zcjBkzvNzf/vY3J96yZYtXc8kllzjx5s2bvZq7777bifv27evVJJ2njO/tSDonGr/ryJo1a7ya+O/olStXejXFjJUkAAABNEkAAAJokgAABNAkAQAIsKSTsf/3TbPwNyFJatWqlZf75JNP6n3e+eef78TFeDGBKIryc1WGPVRJ8+6b3/yml/vFL37h5Xbs2OHExx57rFezevXq/A0sRWnMu0LOuRdffNHLtW/f3stNmjTJiZM292QjPleS7soRv1OIlN3Gnbj//u//9nLDhw+v93lpq2vOsZIEACCAJgkAQABNEgCAgJK7mECxOfvss9MeAkrYF198kVVd/PxQ06ZNG2I4KIB58+Z5uaeeesrLJX0wPxfxD/x37do1q+ddfPHFThy/kEuStWvXZj+wEsFKEgCAAJokAAABNEkAAAJokgAABBTNxp0mTZp4ubPOOsvLxT+IG/+QdUMbOXKkE0+dOrWgx0d5SdrEUVVV5eWOPvpoJx4zZoxXM2rUqPwNDA2mIX9nJF3cZMiQIU7csmVLrybpzhxPPPFE/gZWwlhJAgAQQJMEACCAJgkAQEBq5yR79+7txLfeeqtXc+aZZ3q5jh07OnG+PnB74IEHern+/ft7uXvuuceJW7RoUe9rJ503/etf/7oHo0MlWbhwoZc7/PDDnfg73/lOoYaDEpJ0Xvraa6914o0bN3o1p59+eoONqdSxkgQAIIAmCQBAAE0SAIAAmiQAAAGpbdy59957nTjbK9P/27/9mxNv3bo1L+NJ2iTUo0cPL5fN3blfeuklJ77//vu9msWLF2c/OFS8+LzbtWtXSiNBMWnfvr0TX3nllV5NfO48+OCDXk053r0jX1hJAgAQQJMEACCAJgkAQABNEgCAgKK5C0i24lePKLT41SqeeeYZr+aGG25wYq6ug70Vv3PDoEGDvJo5c+YUajgoEosWLXLi+EYeSZo1a5YTf+9732vQMZUbVpIAAATQJAEACKBJAgAQkNo5yREjRjjx6NGjvZrLL7+8wY4fvxP39u3bvZqlS5d6ufgHcVesWJHfgaHifetb3/JyO3fudOJ33nmnUMNBEZsxY4YTT5w40auZN29eoYZTllhJAgAQQJMEACCAJgkAQABNEgCAAKvrrhZmVv8tL/KkadOmXi6+uUeSJk2a5MRt2rTxaubOnevE8Q/cSv7J7A0bNmQzzIoSRZGlcdxCzrtiNHv2bC93zDHHOPHAgQO9mtWrVzfYmAopjXlX6XOu0tU151hJAgAQQJMEACCAJgkAQEDRnJNE8eGcJNLAOUkUGuckAQDIAU0SAIAAmiQAAAE0SQAAAmiSAAAE0CQBAAigSQIAEECTBAAggCYJAEAATRIAgACaJAAAATRJAAACaJIAAATUeRcQAAAqGStJAAACaJIAAATQJAEACKBJAgAQQJMEACCAJgkAQMD/B25/b+s4+eraAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 查看MNIST数据集\n",
    "import matplotlib.pyplot as plt\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    img = train_dataloader.dataset[i][0]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title('label:'+str(train_dataloader.dataset[i][1]))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 设计MLP结构  \n",
    "    我们的输入时图片的二维形式，首先需要将其扁平化，转换为一维矩阵，我们可以使用view函数，当然nn.Flatten()层也可以帮我们做这件事；  \n",
    "    \n",
    "    网络的主要部分由线性层Linear()和激活函数ReLU()组成，其中线性层要求每层的输入向量大小与上层的输出大小相同，最终的输出大小为类别个数，即10类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "        MLP:共包含1个Flatten层，5个Linear层，激活函数为ReLU\n",
    "    '''\n",
    "    def __init__(self, cls_num):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fully_connected_layers = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, cls_num),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fully_connected_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fully_connected_layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看网络结构\n",
    "model = MLP(10).cuda()\n",
    "print(\"Model structure: \", model, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 定义超参数  \n",
    "学习率：每次反向传播后网络参数更新的步长；  \n",
    "\n",
    "批大小batch_size：mini-batch的大小，batch越大损失函数波动越小，但所需时间和内存更多；  \n",
    "\n",
    "epoch：将训练集全部训练一遍的重复次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.5 损失函数和优化器  \n",
    "损失函数：对于多分类问题我们选择交叉熵；  \n",
    "优化器：一个好的优化器能加快训练，我们使用Adam优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数和优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.6 训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练、测试\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch,  (x, y) in enumerate(dataloader):\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        pred = model(x) # 正向传播\n",
    "        optimizer.zero_grad() # 梯度置零\n",
    "        loss = loss_fn(pred, y) # 计算损失\n",
    "        loss.backward() # 反向传播\n",
    "        optimizer.step() # 更新参数\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*len(x)\n",
    "            print(f'loss:{loss:>7f} [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, accuracy = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            accuracy += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    accuracy /= size\n",
    "    print(f'Accuracy: {(100*accuracy):>0.1f}%, Average Loss: {test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss:0.003964 [    0/60000]\n",
      "loss:0.111697 [ 6400/60000]\n",
      "loss:0.001530 [12800/60000]\n",
      "loss:0.007487 [19200/60000]\n",
      "loss:0.021410 [25600/60000]\n",
      "loss:0.031708 [32000/60000]\n",
      "loss:0.033676 [38400/60000]\n",
      "loss:0.000934 [44800/60000]\n",
      "loss:0.080093 [51200/60000]\n",
      "loss:0.049658 [57600/60000]\n",
      "Accuracy: 97.8%, Average Loss: 0.001699\n",
      "\n",
      "Epoch 1:\n",
      "loss:0.001914 [    0/60000]\n",
      "loss:0.002409 [ 6400/60000]\n",
      "loss:0.006600 [12800/60000]\n",
      "loss:0.000101 [19200/60000]\n",
      "loss:0.043018 [25600/60000]\n",
      "loss:0.001259 [32000/60000]\n",
      "loss:0.006554 [38400/60000]\n",
      "loss:0.052096 [44800/60000]\n",
      "loss:0.005402 [51200/60000]\n",
      "loss:0.006541 [57600/60000]\n",
      "Accuracy: 98.0%, Average Loss: 0.001516\n",
      "\n",
      "Epoch 2:\n",
      "loss:0.003596 [    0/60000]\n",
      "loss:0.049215 [ 6400/60000]\n",
      "loss:0.011896 [12800/60000]\n",
      "loss:0.001325 [19200/60000]\n",
      "loss:0.010697 [25600/60000]\n",
      "loss:0.107927 [32000/60000]\n",
      "loss:0.001971 [38400/60000]\n",
      "loss:0.002728 [44800/60000]\n",
      "loss:0.022347 [51200/60000]\n",
      "loss:0.009136 [57600/60000]\n",
      "Accuracy: 97.9%, Average Loss: 0.001577\n",
      "\n",
      "Epoch 3:\n",
      "loss:0.007822 [    0/60000]\n",
      "loss:0.000537 [ 6400/60000]\n",
      "loss:0.066401 [12800/60000]\n",
      "loss:0.004175 [19200/60000]\n",
      "loss:0.010067 [25600/60000]\n",
      "loss:0.039125 [32000/60000]\n",
      "loss:0.006570 [38400/60000]\n",
      "loss:0.046452 [44800/60000]\n",
      "loss:0.034396 [51200/60000]\n",
      "loss:0.023685 [57600/60000]\n",
      "Accuracy: 98.2%, Average Loss: 0.001494\n",
      "\n",
      "Epoch 4:\n",
      "loss:0.001636 [    0/60000]\n",
      "loss:0.000497 [ 6400/60000]\n",
      "loss:0.004177 [12800/60000]\n",
      "loss:0.085137 [19200/60000]\n",
      "loss:0.000034 [25600/60000]\n",
      "loss:0.002182 [32000/60000]\n",
      "loss:0.000225 [38400/60000]\n",
      "loss:0.024255 [44800/60000]\n",
      "loss:0.002092 [51200/60000]\n",
      "loss:0.004384 [57600/60000]\n",
      "Accuracy: 98.1%, Average Loss: 0.001615\n",
      "\n",
      "Epoch 5:\n",
      "loss:0.026436 [    0/60000]\n",
      "loss:0.075221 [ 6400/60000]\n",
      "loss:0.001631 [12800/60000]\n",
      "loss:0.001343 [19200/60000]\n",
      "loss:0.034542 [25600/60000]\n",
      "loss:0.000200 [32000/60000]\n",
      "loss:0.064244 [38400/60000]\n",
      "loss:0.019392 [44800/60000]\n",
      "loss:0.004014 [51200/60000]\n",
      "loss:0.001042 [57600/60000]\n",
      "Accuracy: 98.2%, Average Loss: 0.001602\n",
      "\n",
      "Epoch 6:\n",
      "loss:0.003597 [    0/60000]\n",
      "loss:0.006142 [ 6400/60000]\n",
      "loss:0.018622 [12800/60000]\n",
      "loss:0.042618 [19200/60000]\n",
      "loss:0.031916 [25600/60000]\n",
      "loss:0.055908 [32000/60000]\n",
      "loss:0.001144 [38400/60000]\n",
      "loss:0.002320 [44800/60000]\n",
      "loss:0.100791 [51200/60000]\n",
      "loss:0.000957 [57600/60000]\n",
      "Accuracy: 98.3%, Average Loss: 0.001445\n",
      "\n",
      "Epoch 7:\n",
      "loss:0.000382 [    0/60000]\n",
      "loss:0.005186 [ 6400/60000]\n",
      "loss:0.040525 [12800/60000]\n",
      "loss:0.002969 [19200/60000]\n",
      "loss:0.000068 [25600/60000]\n",
      "loss:0.001339 [32000/60000]\n",
      "loss:0.063370 [38400/60000]\n",
      "loss:0.010427 [44800/60000]\n",
      "loss:0.002155 [51200/60000]\n",
      "loss:0.023103 [57600/60000]\n",
      "Accuracy: 98.0%, Average Loss: 0.001365\n",
      "\n",
      "Epoch 8:\n",
      "loss:0.000486 [    0/60000]\n",
      "loss:0.002774 [ 6400/60000]\n",
      "loss:0.008002 [12800/60000]\n",
      "loss:0.013395 [19200/60000]\n",
      "loss:0.034496 [25600/60000]\n",
      "loss:0.003239 [32000/60000]\n",
      "loss:0.001558 [38400/60000]\n",
      "loss:0.000635 [44800/60000]\n",
      "loss:0.014488 [51200/60000]\n",
      "loss:0.001022 [57600/60000]\n",
      "Accuracy: 97.7%, Average Loss: 0.002244\n",
      "\n",
      "Epoch 9:\n",
      "loss:0.003624 [    0/60000]\n",
      "loss:0.027028 [ 6400/60000]\n",
      "loss:0.003526 [12800/60000]\n",
      "loss:0.001227 [19200/60000]\n",
      "loss:0.000054 [25600/60000]\n",
      "loss:0.003085 [32000/60000]\n",
      "loss:0.000425 [38400/60000]\n",
      "loss:0.001661 [44800/60000]\n",
      "loss:0.000842 [51200/60000]\n",
      "loss:0.000409 [57600/60000]\n",
      "Accuracy: 98.0%, Average Loss: 0.001813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#执行每个epoch\n",
    "for i in range(epoch):\n",
    "    print(f'Epoch {i}:')\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.0%, Average Loss: 0.001813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 在测试集上测试准确率 98.0%\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorbase] *",
   "language": "python",
   "name": "conda-env-tensorbase-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
