# 5. 循环神经网络RNN

循环神经网络（RNN）由于其特有的循环结构，非常适合处理序列型数据，RNN的参数对于整个序列数据共享，因此它可以从数据中提取时序信息。由于RNN的时序处理能力，它常被用于翻译、语音识别等自然语言处理领域。

## 5.1 RNN

### 5.1.1 RNN的结构	

RNN也属于神经网络，基本结构包含输入层、输出层、隐藏层，不同的是它包含一个循环结构：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710104037967.png" alt="image-20210710104037967" style="zoom:80%;" />

​	如果我们将RNN展开，就得到了下图，可以看到每次有了新的输入x，隐藏层的信息A都会把当前状态向下一个状态传播，这也是RNN包含序列信息的原因：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710104200532.png" alt="image-20210710104200532" style="zoom:80%;" />

​	我们接着观察RNN的内部结构：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710104523070.png" alt="image-20210710104523070" style="zoom:80%;" />

​	RNN的参数包含3部分$U:输入到隐藏层, V:隐藏层到输出层, W:隐藏层到隐藏层$，每个新的输入与$U$相乘，上个状态的隐藏层与$W$相乘，二者相加得到当前状态的隐藏层，隐藏层输出一方面经过激活函数，与$V$相乘再到输出层，另一方面向下一个状态传播。

### 5.1.2 RNN的前向传播与反向传播算法

前向传播：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710105400694.png" alt="image-20210710105400694" style="zoom:67%;" />

反向传播：**BPTT(Back Prropagation Through TIme)**

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710105443302.png" alt="image-20210710105443302" style="zoom:67%;" />

## 5.2 LSTM

### 5.2.1 RNN存在的问题：长依赖关系Long-Term Dependencies

RNN可以将以前的信息连接到当前的任务中，但它并不总是有效的。由于RNN的链式结构，随着图中间隔的增大，以前的信息将会不断减弱，也就是说网络会逐渐遗忘以前的信息。例如，“I grew up in France… I speak fluent *French*”，显然France和French有很强的关联关系，但由于其间隔较远，网络将会逐渐遗忘France，而fluent将会是影响下一个单词最重要的因素。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710105913168.png" alt="image-20210710105913168" style="zoom:80%;" />

### 5.2.2 LSTM的原理

RNN会将每个状态的信息全部保存下来传播到下一状态，而LSTM引入了门 Gate的概念，门可以允许网络去选择保留什么信息。如下图Memory Cell是中间记忆，每次输入首先通过输入门选择存储，当储存到Memory Cell后，网络会通过遗忘门选择性保留有用的信息。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710110433529.png" alt="image-20210710110433529" style="zoom:80%;" />

### 5.2.3 LSTM的结构

相比RNN，LSTM有更复杂的循环结构：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710111042387.png" alt="image-20210710111042387" style="zoom:70%;" />

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710111125350.png" alt="image-20210710111125350" style="zoom:67%;" />

LSTM的核心是细胞状态Cell State，它包含了之前状态的信息，是网络的记忆：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710111311470.png" alt="image-20210710111311470" style="zoom:67%;" />

LSTM通过门来选择哪些信息继续传播，通常门是一个sigmoid网络层，注意sigmoid的输出在01之间，因此通过将门与细胞状态相乘，有用的信息被保留，而无用信息与很小的数相乘，被消除。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710111718651.png" alt="image-20210710111718651" style="zoom:67%;" />

### 5.2.4 LSTM的前向传播过程

首先网络根据上一个状态的信息和新信息计算遗忘门，遗忘门也是一个sigmoid网络层，当遗忘门与上一个细胞状态相乘，就选择性保留了以前的信息：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710112037809.png" alt="image-20210710112037809" style="zoom:67%;" />

接着要选择保留当前状态的信息，包括两个部分。首先输入门，一个sigmoid层，选择哪些值被更新，另一个tanh层用于生成更新细胞，二者相乘就保留所需信息，再与以前的信息相加就完成了当前的细胞状态。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710112722624.png" alt="image-20210710112722624" style="zoom:67%;" />![image-20210710112746991](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710112746991.png)

所以细胞状态的更新包括：与遗忘门相乘，再与更新细胞相加。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710112817019.png" alt="image-20210710112817019" style="zoom:67%;" />

输出时细胞状态先通过一个tanh门，被压缩至-1到1之间，再与通过sigmoid门的输入相乘：

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710113225392.png" alt="image-20210710113225392" style="zoom:67%;" />

## 5.3 GRU

GRU是LSTM的一种变体，它将遗忘门和输入门合成为更新门，参数相比LSTM更少，但准确率相近，过拟合的可能更小。

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20210710113607169.png" alt="image-20210710113607169" style="zoom:67%;" />