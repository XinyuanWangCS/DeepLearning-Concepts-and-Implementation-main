{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 2611,
     "status": "ok",
     "timestamp": 1624151921475,
     "user": {
      "displayName": "王心远",
      "photoUrl": "",
      "userId": "05984323232349612671"
     },
     "user_tz": -480
    },
    "id": "HoSM4vm9LfTV"
   },
   "source": [
    "# 5. 循环神经网络RNN\n",
    "\n",
    "循环神经网络（RNN）由于其特有的循环结构，非常适合处理序列型数据，RNN的参数对于整个序列数据共享，因此它可以从数据中提取时序信息。由于RNN的时序处理能力，它常被用于翻译、语音识别等自然语言处理领域。\n",
    "\n",
    "## 5.1 RNN\n",
    "\n",
    "### 5.1.1 RNN的结构\t\n",
    "\n",
    "RNN也属于神经网络，基本结构包含输入层、输出层、隐藏层，不同的是它包含一个循环结构：\n",
    "\n",
    "<img src=\"./imgs/图片1.png\" alt=\"image-20210710104037967\" style=\"zoom:80%;\" />\n",
    "\n",
    "​\t如果我们将RNN展开，就得到了下图，可以看到每次有了新的输入x，隐藏层的信息A都会把当前状态向下一个状态传播，这也是RNN包含序列信息的原因：\n",
    "\n",
    "<img src=\"./imgs/图片2.png\" alt=\"image-20210710104200532\" style=\"zoom:80%;\" />\n",
    "\n",
    "​\t我们接着观察RNN的内部结构：\n",
    "\n",
    "<img src=\"./imgs/图片3.png\" alt=\"image-20210710104523070\" style=\"zoom:80%;\" />\n",
    "\n",
    "​\tRNN的参数包含3部分$U:输入到隐藏层, V:隐藏层到输出层, W:隐藏层到隐藏层$，每个新的输入与$U$相乘，上个状态的隐藏层与$W$相乘，二者相加得到当前状态的隐藏层，隐藏层输出一方面经过激活函数，与$V$相乘再到输出层，另一方面向下一个状态传播。\n",
    "\n",
    "### 5.1.2 RNN的前向传播与反向传播算法\n",
    "\n",
    "前向传播：\n",
    "\n",
    "<img src=\"./imgs/图片4.png\" alt=\"image-20210710105400694\" style=\"zoom:67%;\" />\n",
    "\n",
    "反向传播：**BPTT(Back Prropagation Through TIme)**\n",
    "\n",
    "<img src=\"./imgs/图片5.png\" alt=\"image-20210710105443302\" style=\"zoom:67%;\" />\n",
    "\n",
    "## 5.2 LSTM\n",
    "\n",
    "### 5.2.1 RNN存在的问题：长依赖关系Long-Term Dependencies\n",
    "\n",
    "RNN可以将以前的信息连接到当前的任务中，但它并不总是有效的。由于RNN的链式结构，随着图中间隔的增大，以前的信息将会不断减弱，也就是说网络会逐渐遗忘以前的信息。例如，“I grew up in France… I speak fluent *French*”，显然France和French有很强的关联关系，但由于其间隔较远，网络将会逐渐遗忘France，而fluent将会是影响下一个单词最重要的因素。\n",
    "\n",
    "<img src=\"./imgs/图片6.png\" alt=\"image-20210710105913168\" style=\"zoom:80%;\" />\n",
    "\n",
    "### 5.2.2 LSTM的原理\n",
    "\n",
    "RNN会将每个状态的信息全部保存下来传播到下一状态，而LSTM引入了门 Gate的概念，门可以允许网络去选择保留什么信息。如下图Memory Cell是中间记忆，每次输入首先通过输入门选择存储，当储存到Memory Cell后，网络会通过遗忘门选择性保留有用的信息。\n",
    "\n",
    "<img src=\"./imgs/图片7.png\" alt=\"image-20210710110433529\" style=\"zoom:80%;\" />\n",
    "\n",
    "### 5.2.3 LSTM的结构\n",
    "\n",
    "相比RNN，LSTM有更复杂的循环结构：\n",
    "\n",
    "<img src=\"./imgs/图片8.png\" alt=\"image-20210710111042387\" style=\"zoom:70%;\" />\n",
    "\n",
    "<img src=\"./imgs/图片9.png\" alt=\"image-20210710111125350\" style=\"zoom:67%;\" />\n",
    "\n",
    "LSTM的核心是细胞状态Cell State，它包含了之前状态的信息，是网络的记忆：\n",
    "\n",
    "<img src=\"./imgs/图片10.png\" alt=\"image-20210710111311470\" style=\"zoom:67%;\" />\n",
    "\n",
    "LSTM通过门来选择哪些信息继续传播，通常门是一个sigmoid网络层，注意sigmoid的输出在01之间，因此通过将门与细胞状态相乘，有用的信息被保留，而无用信息与很小的数相乘，被消除。\n",
    "\n",
    "<img src=\"./imgs/图片11.png\" alt=\"image-20210710111718651\" style=\"zoom:67%;\" />\n",
    "\n",
    "### 5.2.4 LSTM的前向传播过程\n",
    "\n",
    "首先网络根据上一个状态的信息和新信息计算遗忘门，遗忘门也是一个sigmoid网络层，当遗忘门与上一个细胞状态相乘，就选择性保留了以前的信息：\n",
    "\n",
    "<img src=\"./imgs/图片12.png\" alt=\"image-20210710112037809\" style=\"zoom:67%;\" />\n",
    "\n",
    "接着要选择保留当前状态的信息，包括两个部分。首先输入门，一个sigmoid层，选择哪些值被更新，另一个tanh层用于生成更新细胞，二者相乘就保留所需信息，再与以前的信息相加就完成了当前的细胞状态。\n",
    "\n",
    "<img src=\"./imgs/图片13.png\" alt=\"image-20210710112722624\" style=\"zoom:67%;\" />\n",
    "\n",
    "所以细胞状态的更新包括：与遗忘门相乘，再与更新细胞相加。\n",
    "\n",
    "<img src=\"./imgs/图片14.png\" alt=\"image-20210710112817019\" style=\"zoom:67%;\" />\n",
    "\n",
    "输出时细胞状态先通过一个tanh门，被压缩至-1到1之间，再与通过sigmoid门的输入相乘：\n",
    "\n",
    "<img src=\"./imgs/图片15.png\" alt=\"image-20210710113225392\" style=\"zoom:67%;\" />\n",
    "\n",
    "## 5.3 GRU\n",
    "\n",
    "GRU是LSTM的一种变体，它将遗忘门和输入门合成为更新门，参数相比LSTM更少，但准确率相近，过拟合的可能更小。\n",
    "\n",
    "<img src=\"./imgs/图片16.png\" alt=\"image-20210710113607169\" style=\"zoom:67%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 使用LSTM完成推特情感分析  \n",
    "本部分使用Keras实现推特数据集情感分类任务。  \n",
    "数据集：nltk的推特情感分析语料库，所有推特分为开心1和不开心0两类，共10000条数据。  \n",
    "环境：keras，keras里面有Tokenizer等包可以很方便地处理文字数据，模型实现也比较简单。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 导入需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 下载语料库  \n",
    "twitter_samples是推特数据集，每条推特的标签为0不开心，1开心  \n",
    "\n",
    "stopwords是停词数据，停词是对语义无大影响的单词，比如is，a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1308,
     "status": "ok",
     "timestamp": 1624151922781,
     "user": {
      "displayName": "王心远",
      "photoUrl": "",
      "userId": "05984323232349612671"
     },
     "user_tz": -480
    },
    "id": "o-C4ALrjMGka",
    "outputId": "cb851272-ee0d-40ea-804f-a275a7a74b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下载语料库\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.4.3 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1624151924905,
     "user": {
      "displayName": "王心远",
      "photoUrl": "",
      "userId": "05984323232349612671"
     },
     "user_tz": -480
    },
    "id": "L4d57XMzLZQB"
   },
   "outputs": [],
   "source": [
    "# 把推特分为训练集和测试集\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1624152942063,
     "user": {
      "displayName": "王心远",
      "photoUrl": "",
      "userId": "05984323232349612671"
     },
     "user_tz": -480
    },
    "id": "ITgZp4EgDivz",
    "outputId": "abc3b919-7ed9-46a6-8616-6ad487aac6dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
       " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
       " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
       " '@97sides CONGRATS :)',\n",
       " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看部分推特\n",
    "train_pos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1624152945048,
     "user": {
      "displayName": "王心远",
      "photoUrl": "",
      "userId": "05984323232349612671"
     },
     "user_tz": -480
    },
    "id": "E_rrWhTXDykZ",
    "outputId": "c6872dff-7302-41de-8e4a-1d84cb768dbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hopeless for tmr :(',\n",
       " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
       " '@Hegelbon That heart sliding into the waste basket. :(',\n",
       " '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too',\n",
       " 'Dang starting next week I have \"work\" :(']"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.4 去除停词和stemming  \n",
    "去除停词可以使句子在不影响语义的情况下变得更短；  \n",
    "\n",
    "stemming是在英语中将同一单词的不同形式，如第三人称等都进行统一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "executionInfo": {
     "elapsed": 807,
     "status": "ok",
     "timestamp": 1624151927659,
     "user": {
      "displayName": "王心远",
      "photoUrl": "",
      "userId": "05984323232349612671"
     },
     "user_tz": -480
    },
    "id": "dTVB1bHKLTW6",
    "outputId": "295d1240-8b5b-4b00-a54a-f3cd24d505db"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>followfriday franc int pkuchly57 milipol pari ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey jame odd pleas call contact centr 02392441...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>listen last night bleed amaz track scotland</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>congrat</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppi accnt verifi rqst succeed got ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  label\n",
       "0  followfriday franc int pkuchly57 milipol pari ...    1.0\n",
       "1  hey jame odd pleas call contact centr 02392441...    1.0\n",
       "2        listen last night bleed amaz track scotland    1.0\n",
       "3                                            congrat    1.0\n",
       "4  yeaaaah yippppi accnt verifi rqst succeed got ...    1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去除停词和stemming\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "def preprocess(text, stem=False):\n",
    "  \n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df = pd.DataFrame({'data':train_x+test_x, 'label':np.concatenate((train_y,test_y))})\n",
    "df.data = df.data.apply(lambda x: preprocess(x,True))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.5 Tokenizer  \n",
    "Tokenizer首先将文本数据转化为一个单词和频率的词典，如I 10000， am 500;  \n",
    "\n",
    "然后根据字典的下标，Tokenizer可以将文本的句子转化为向量，如 I love you->[2 50 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1624151929316,
     "user": {
      "displayName": "王心远",
      "photoUrl": "",
      "userId": "05984323232349612671"
     },
     "user_tz": -480
    },
    "id": "dW6SUem_KYwu",
    "outputId": "17336159-9295-44e0-bd04-89b0a3ac75e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 21573\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_x+test_x)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size)\n",
    "\n",
    "SEQUENCE_LENGTH = 200\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(df.data.iloc[:8000]), maxlen=SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(df.data.iloc[8000:]), maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.6 定义模型  \n",
    "我们的模型包括一个Embedding层，可以将数据降维至所需长度；  \n",
    "一层LSTM层；  \n",
    "一层全连接层用于二分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5478,
     "status": "ok",
     "timestamp": 1624151946455,
     "user": {
      "displayName": "王心远",
      "photoUrl": "",
      "userId": "05984323232349612671"
     },
     "user_tz": -480
    },
    "id": "nshGG2GgLZQL",
    "outputId": "b62ef51b-a114-498a-a62f-7c1ff0f2944c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 200)          4314600   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,435,101\n",
      "Trainable params: 4,435,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 定义模型：LSTM功能比较强大，数据集较小就只用了一个LSTM层\n",
    "maxlen = 21573\n",
    "embed_dim = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(maxlen, embed_dim, input_length = 200)) # Embedding数据降维\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) # LSTM\n",
    "model.add(Dense(1,activation='sigmoid')) # 全连接 2分类\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"]) #binary_crossentropy\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.7 结果\n",
    "训练了10个epoch，训练集准确率为94.3%，测试集准确率为66.8%，有一定的过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 881298,
     "status": "ok",
     "timestamp": 1624152830461,
     "user": {
      "displayName": "王心远",
      "photoUrl": "",
      "userId": "05984323232349612671"
     },
     "user_tz": -480
    },
    "id": "txitxtB8Dmib",
    "outputId": "1b7286be-cc8e-4996-9484-fd6aabb7c19a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 104s 727ms/step - loss: 0.6570 - accuracy: 0.5780 - val_loss: 0.6833 - val_accuracy: 0.6175\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 81s 717ms/step - loss: 0.4132 - accuracy: 0.8129 - val_loss: 0.6823 - val_accuracy: 0.6712\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 83s 731ms/step - loss: 0.3171 - accuracy: 0.8608 - val_loss: 0.8245 - val_accuracy: 0.6313\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 81s 719ms/step - loss: 0.2425 - accuracy: 0.8953 - val_loss: 0.9117 - val_accuracy: 0.6513\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 81s 715ms/step - loss: 0.2035 - accuracy: 0.9139 - val_loss: 0.8578 - val_accuracy: 0.6837\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 81s 715ms/step - loss: 0.1869 - accuracy: 0.9183 - val_loss: 1.0863 - val_accuracy: 0.6450\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 82s 723ms/step - loss: 0.1587 - accuracy: 0.9311 - val_loss: 1.3137 - val_accuracy: 0.6162\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 81s 716ms/step - loss: 0.1496 - accuracy: 0.9302 - val_loss: 1.5746 - val_accuracy: 0.6350\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 81s 714ms/step - loss: 0.1228 - accuracy: 0.9425 - val_loss: 1.4909 - val_accuracy: 0.6363\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 81s 720ms/step - loss: 0.1132 - accuracy: 0.9483 - val_loss: 1.4667 - val_accuracy: 0.5913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb53948f450>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, train_y, batch_size=64, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1731,
     "status": "ok",
     "timestamp": 1624152832177,
     "user": {
      "displayName": "王心远",
      "photoUrl": "",
      "userId": "05984323232349612671"
     },
     "user_tz": -480
    },
    "id": "29aG0jSoMqV-",
    "outputId": "d4086900-7a40-4b83-e86d-42dc8f2c3a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 50ms/step - loss: 1.2070 - accuracy: 0.6675\n",
      "\n",
      "ACCURACY: 0.6675000190734863\n",
      "LOSS: 1.206989049911499\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, test_y, batch_size=64)\n",
    "print()\n",
    "print(\"ACCURACY:\",score[1])\n",
    "print(\"LOSS:\",score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_fUM5k1Rdhn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "循环神经网络RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tensorbase] *",
   "language": "python",
   "name": "conda-env-tensorbase-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
